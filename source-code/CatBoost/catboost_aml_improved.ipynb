{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7769d7c4",
   "metadata": {},
   "source": [
    "# Anti-Money Laundering Detection with CatBoost\n",
    "\n",
    "Notebook ini melakukan deteksi transaksi pencucian uang (money laundering) menggunakan **CatBoost** sebagai alternatif dari pendekatan Graph Neural Network (Multi-GNN).\n",
    "\n",
    "Pipeline:\n",
    "1. Load data (`formatted_transactions.csv`)\n",
    "2. Feature Engineering (graph-based, temporal, statistical)\n",
    "3. Temporal split (60/20/20)\n",
    "4. Training CatBoost dengan class weighting\n",
    "5. Evaluasi: F1, Precision, Recall, PR-AUC, Confusion Matrix\n",
    "6. Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb1da13",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score,\n",
    "    average_precision_score, confusion_matrix,\n",
    "    classification_report, precision_recall_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('Libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ec646",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e64e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load raw CSV dan preprocessing\n",
    "# ============================================================\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_PATH = './HI-Small_Trans.csv'\n",
    "raw = pd.read_csv(DATA_PATH, dtype=str)\n",
    "\n",
    "print(f'Raw dataset shape: {raw.shape}')\n",
    "print(f'Raw columns: {raw.columns.tolist()}')\n",
    "\n",
    "# --- Encode categorical columns to integer IDs ---\n",
    "currency_map = {}\n",
    "payment_format_map = {}\n",
    "account_map = {}\n",
    "\n",
    "def get_dict_val(name, collection):\n",
    "    if name in collection:\n",
    "        return collection[name]\n",
    "    val = len(collection)\n",
    "    collection[name] = val\n",
    "    return val\n",
    "\n",
    "# --- Build processed dataframe ---\n",
    "records = []\n",
    "first_ts = None\n",
    "\n",
    "for i in range(len(raw)):\n",
    "    row = raw.iloc[i]\n",
    "    \n",
    "    # Parse timestamp\n",
    "    dt = datetime.strptime(row['Timestamp'], '%Y/%m/%d %H:%M')\n",
    "    ts = dt.timestamp()\n",
    "    \n",
    "    if first_ts is None:\n",
    "        start_time = datetime(dt.year, dt.month, dt.day)\n",
    "        first_ts = start_time.timestamp() - 10\n",
    "    \n",
    "    ts_relative = ts - first_ts\n",
    "    \n",
    "    # Encode accounts: Bank + Account â†’ unique integer ID\n",
    "    from_acc_str = str(row['From Bank']) + str(row.iloc[2])  # From Bank + Account\n",
    "    to_acc_str = str(row['To Bank']) + str(row.iloc[4])      # To Bank + Account (2nd Account col)\n",
    "    from_id = get_dict_val(from_acc_str, account_map)\n",
    "    to_id = get_dict_val(to_acc_str, account_map)\n",
    "    \n",
    "    # Encode currencies\n",
    "    recv_currency = get_dict_val(row['Receiving Currency'], currency_map)\n",
    "    pay_currency = get_dict_val(row['Payment Currency'], currency_map)\n",
    "    \n",
    "    # Encode payment format\n",
    "    pay_format = get_dict_val(row['Payment Format'], payment_format_map)\n",
    "    \n",
    "    # Amounts\n",
    "    amount_received = float(row['Amount Received'])\n",
    "    amount_paid = float(row['Amount Paid'])\n",
    "    \n",
    "    # Label\n",
    "    is_laundering = int(row['Is Laundering'])\n",
    "    \n",
    "    records.append({\n",
    "        'from_id': from_id,\n",
    "        'to_id': to_id,\n",
    "        'Timestamp': ts_relative,\n",
    "        'Amount Paid': amount_paid,\n",
    "        'Payment Currency': pay_currency,\n",
    "        'Amount Received': amount_received,\n",
    "        'Receiving Currency': recv_currency,\n",
    "        'Payment Format': pay_format,\n",
    "        'Is Laundering': is_laundering,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df = df.sort_values('Timestamp').reset_index(drop=True)\n",
    "\n",
    "print(f'\\nProcessed dataset shape: {df.shape}')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "print(f'\\nClass distribution:')\n",
    "print(df['Is Laundering'].value_counts())\n",
    "print(f'\\nIllicit ratio: {df[\"Is Laundering\"].mean() * 100:.2f}%')\n",
    "print(f'\\nCurrency mapping: {currency_map}')\n",
    "print(f'Payment format mapping: {payment_format_map}')\n",
    "print(f'Number of unique accounts: {len(account_map)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7282ddb",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Kita buat fitur-fitur yang menangkap informasi serupa dengan yang digunakan Multi-GNN:\n",
    "- **Edge features dasar**: Amount, Currency, Payment Format\n",
    "- **Graph-based features**: in/out degree, PageRank-like\n",
    "- **Port numbering features**: urutan koneksi unik per node\n",
    "- **Time delta features**: selisih waktu antar transaksi\n",
    "- **Aggregated node features**: statistik transaksi per akun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d262276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalize timestamp (relatif ke minimum) ---\n",
    "df['Timestamp'] = df['Timestamp'] - df['Timestamp'].min()\n",
    "\n",
    "# Sort by timestamp untuk temporal features\n",
    "df = df.sort_values('Timestamp').reset_index(drop=True)\n",
    "\n",
    "print(f'Timestamp range: {df[\"Timestamp\"].min()} - {df[\"Timestamp\"].max()}')\n",
    "print(f'Number of unique senders (from_id): {df[\"from_id\"].nunique()}')\n",
    "print(f'Number of unique receivers (to_id): {df[\"to_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.1 Graph-based Features (Node Degree & Transaction Stats)\n",
    "# ============================================================\n",
    "\n",
    "# --- Degree features ---\n",
    "out_degree = df.groupby('from_id').size().reset_index(name='from_out_degree')\n",
    "in_degree = df.groupby('to_id').size().reset_index(name='to_in_degree')\n",
    "\n",
    "df = df.merge(out_degree, on='from_id', how='left')\n",
    "df = df.merge(in_degree, on='to_id', how='left')\n",
    "\n",
    "# In-degree of sender, Out-degree of receiver (cross features)\n",
    "from_in_degree = df.groupby('to_id').size().reset_index(name='from_in_degree')\n",
    "from_in_degree.columns = ['from_id', 'from_in_degree']\n",
    "to_out_degree = df.groupby('from_id').size().reset_index(name='to_out_degree')\n",
    "to_out_degree.columns = ['to_id', 'to_out_degree']\n",
    "\n",
    "df = df.merge(from_in_degree, on='from_id', how='left')\n",
    "df = df.merge(to_out_degree, on='to_id', how='left')\n",
    "df['from_in_degree'] = df['from_in_degree'].fillna(0)\n",
    "df['to_out_degree'] = df['to_out_degree'].fillna(0)\n",
    "\n",
    "# Total degree\n",
    "df['from_total_degree'] = df['from_out_degree'] + df['from_in_degree']\n",
    "df['to_total_degree'] = df['to_in_degree'] + df['to_out_degree']\n",
    "\n",
    "# --- Degree ratio (asymmetry) ---\n",
    "df['from_degree_ratio'] = df['from_out_degree'] / (df['from_in_degree'] + 1)\n",
    "df['to_degree_ratio'] = df['to_in_degree'] / (df['to_out_degree'] + 1)\n",
    "\n",
    "# --- Neighbor diversity (unique counterparties / total transactions) ---\n",
    "from_unique = df.groupby('from_id')['to_id'].nunique().reset_index(name='from_unique_neighbors')\n",
    "to_unique = df.groupby('to_id')['from_id'].nunique().reset_index(name='to_unique_neighbors')\n",
    "df = df.merge(from_unique, on='from_id', how='left')\n",
    "df = df.merge(to_unique, on='to_id', how='left')\n",
    "\n",
    "df['from_neighbor_diversity'] = df['from_unique_neighbors'] / (df['from_out_degree'] + 1)\n",
    "df['to_neighbor_diversity'] = df['to_unique_neighbors'] / (df['to_in_degree'] + 1)\n",
    "\n",
    "# --- Fan-out / Fan-in patterns (important for layering detection) ---\n",
    "# How many unique receivers does the sender have vs how many unique senders does the receiver have\n",
    "df['fanout_fanin_ratio'] = df['from_unique_neighbors'] / (df['to_unique_neighbors'] + 1)\n",
    "\n",
    "print('Graph-based features added.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045bc5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.2 Aggregated Amount Features per Node\n",
    "# ============================================================\n",
    "\n",
    "# Sender statistics (paid amounts)\n",
    "from_amount_stats = df.groupby('from_id')['Amount Paid'].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'sum', 'median']\n",
    ").reset_index()\n",
    "from_amount_stats.columns = ['from_id', 'from_amt_mean', 'from_amt_std', \n",
    "                              'from_amt_min', 'from_amt_max', 'from_amt_sum', 'from_amt_median']\n",
    "from_amount_stats['from_amt_std'] = from_amount_stats['from_amt_std'].fillna(0)\n",
    "\n",
    "# Receiver statistics (received amounts)\n",
    "to_amount_stats = df.groupby('to_id')['Amount Received'].agg(\n",
    "    ['mean', 'std', 'min', 'max', 'sum', 'median']\n",
    ").reset_index()\n",
    "to_amount_stats.columns = ['to_id', 'to_amt_mean', 'to_amt_std',\n",
    "                            'to_amt_min', 'to_amt_max', 'to_amt_sum', 'to_amt_median']\n",
    "to_amount_stats['to_amt_std'] = to_amount_stats['to_amt_std'].fillna(0)\n",
    "\n",
    "df = df.merge(from_amount_stats, on='from_id', how='left')\n",
    "df = df.merge(to_amount_stats, on='to_id', how='left')\n",
    "\n",
    "# --- Amount ratio features ---\n",
    "df['amount_diff'] = df['Amount Received'] - df['Amount Paid']\n",
    "df['amount_ratio'] = df['Amount Received'] / (df['Amount Paid'] + 1e-8)\n",
    "df['amount_diff_abs'] = df['amount_diff'].abs()\n",
    "df['amount_log_ratio'] = np.log1p(df['Amount Received']) - np.log1p(df['Amount Paid'])\n",
    "\n",
    "# How much this transaction deviates from sender's/receiver's average\n",
    "df['from_amt_zscore'] = (df['Amount Paid'] - df['from_amt_mean']) / (df['from_amt_std'] + 1e-8)\n",
    "df['to_amt_zscore'] = (df['Amount Received'] - df['to_amt_mean']) / (df['to_amt_std'] + 1e-8)\n",
    "\n",
    "# Transaction amount as fraction of total volume for that account\n",
    "df['from_amt_frac'] = df['Amount Paid'] / (df['from_amt_sum'] + 1e-8)\n",
    "df['to_amt_frac'] = df['Amount Received'] / (df['to_amt_sum'] + 1e-8)\n",
    "\n",
    "# --- Log amounts (reduce skewness) ---\n",
    "df['log_amount_paid'] = np.log1p(df['Amount Paid'])\n",
    "df['log_amount_received'] = np.log1p(df['Amount Received'])\n",
    "\n",
    "# --- Sender avg sent vs receiver avg received (cross-node) ---\n",
    "df['cross_amt_ratio'] = df['from_amt_mean'] / (df['to_amt_mean'] + 1e-8)\n",
    "\n",
    "print('Amount features added.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba684be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.3 Port Numbering Features (sama seperti Multi-GNN --ports)\n",
    "# ============================================================\n",
    "# Port = urutan neighbor unik berdasarkan waktu\n",
    "\n",
    "def compute_ports(df):\n",
    "    \"\"\"Compute port numberings: urutan koneksi unik per node, mirip Multi-GNN.\"\"\"\n",
    "    df_sorted = df.sort_values('Timestamp')\n",
    "    \n",
    "    # In-ports: untuk setiap to_id, berikan nomor urut ke setiap from_id unik\n",
    "    in_ports = []\n",
    "    to_port_map = {}  # to_id -> {from_id: port_number}\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        to_node = row['to_id']\n",
    "        from_node = row['from_id']\n",
    "        if to_node not in to_port_map:\n",
    "            to_port_map[to_node] = {}\n",
    "        if from_node not in to_port_map[to_node]:\n",
    "            to_port_map[to_node][from_node] = len(to_port_map[to_node])\n",
    "        in_ports.append(to_port_map[to_node][from_node])\n",
    "    \n",
    "    # Out-ports: untuk setiap from_id, berikan nomor urut ke setiap to_id unik\n",
    "    out_ports = []\n",
    "    from_port_map = {}  # from_id -> {to_id: port_number}\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        from_node = row['from_id']\n",
    "        to_node = row['to_id']\n",
    "        if from_node not in from_port_map:\n",
    "            from_port_map[from_node] = {}\n",
    "        if to_node not in from_port_map[from_node]:\n",
    "            from_port_map[from_node][to_node] = len(from_port_map[from_node])\n",
    "        out_ports.append(from_port_map[from_node][to_node])\n",
    "    \n",
    "    df_sorted['in_port'] = in_ports\n",
    "    df_sorted['out_port'] = out_ports\n",
    "    return df_sorted\n",
    "\n",
    "df = compute_ports(df)\n",
    "print('Port numbering features added: in_port, out_port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c63737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.4 Time Delta Features (sama seperti Multi-GNN --tds)\n",
    "# ============================================================\n",
    "\n",
    "def compute_time_deltas(df):\n",
    "    \"\"\"Compute time deltas between consecutive transactions per node, mirip Multi-GNN.\"\"\"\n",
    "    df_sorted = df.sort_values('Timestamp').copy()\n",
    "    \n",
    "    # Time delta for incoming transactions (per to_id)\n",
    "    df_sorted['in_time_delta'] = df_sorted.groupby('to_id')['Timestamp'].diff().fillna(0)\n",
    "    \n",
    "    # Time delta for outgoing transactions (per from_id)\n",
    "    df_sorted['out_time_delta'] = df_sorted.groupby('from_id')['Timestamp'].diff().fillna(0)\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "df = compute_time_deltas(df)\n",
    "print('Time delta features added: in_time_delta, out_time_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dcfc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.5 Additional Temporal & Behavioral Features\n",
    "# ============================================================\n",
    "\n",
    "# Day of the transaction\n",
    "df['day'] = (df['Timestamp'] // (3600 * 24)).astype(int)\n",
    "\n",
    "# Hour of the day (cyclical encoding)\n",
    "df['hour'] = ((df['Timestamp'] % (3600 * 24)) // 3600).astype(int)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "# Is self-transaction (from_id == to_id)?\n",
    "df['is_self_tx'] = (df['from_id'] == df['to_id']).astype(int)\n",
    "\n",
    "# Same currency?\n",
    "df['same_currency'] = (df['Payment Currency'] == df['Receiving Currency']).astype(int)\n",
    "\n",
    "# --- Behavioral: transaction velocity per account ---\n",
    "# Number of transactions per day for sender\n",
    "from_daily_count = df.groupby(['from_id', 'day']).size().reset_index(name='from_daily_tx_count')\n",
    "from_daily_avg = from_daily_count.groupby('from_id')['from_daily_tx_count'].mean().reset_index(name='from_avg_daily_tx')\n",
    "df = df.merge(from_daily_avg, on='from_id', how='left')\n",
    "\n",
    "# Number of transactions per day for receiver\n",
    "to_daily_count = df.groupby(['to_id', 'day']).size().reset_index(name='to_daily_tx_count')\n",
    "to_daily_avg = to_daily_count.groupby('to_id')['to_daily_tx_count'].mean().reset_index(name='to_avg_daily_tx')\n",
    "df = df.merge(to_daily_avg, on='to_id', how='left')\n",
    "\n",
    "# --- Sender-Receiver pair features ---\n",
    "pair_count = df.groupby(['from_id', 'to_id']).size().reset_index(name='pair_tx_count')\n",
    "df = df.merge(pair_count, on=['from_id', 'to_id'], how='left')\n",
    "\n",
    "pair_amt = df.groupby(['from_id', 'to_id'])['Amount Paid'].agg(['mean', 'sum']).reset_index()\n",
    "pair_amt.columns = ['from_id', 'to_id', 'pair_amt_mean', 'pair_amt_sum']\n",
    "df = df.merge(pair_amt, on=['from_id', 'to_id'], how='left')\n",
    "\n",
    "# Is this a repeated relationship?\n",
    "df['is_repeat_pair'] = (df['pair_tx_count'] > 1).astype(int)\n",
    "\n",
    "# --- Round amount flag (common in laundering) ---\n",
    "df['is_round_amount'] = ((df['Amount Paid'] % 100 == 0) | (df['Amount Paid'] % 1000 == 0)).astype(int)\n",
    "\n",
    "print('Temporal & behavioral features added.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda37d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.6 Summary of All Features\n",
    "# ============================================================\n",
    "\n",
    "# Define feature columns (exclude identifiers and label)\n",
    "exclude_cols = ['from_id', 'to_id', 'Is Laundering']\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "print(f'Total features: {len(feature_cols)}')\n",
    "print(f'Features: {feature_cols}')\n",
    "print(f'\\nDataset shape: {df.shape}')\n",
    "df[feature_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea4e7c",
   "metadata": {},
   "source": [
    "## 4. Temporal Data Split (60/20/20)\n",
    "\n",
    "Menggunakan **split temporal** yang sama persis dengan Multi-GNN:\n",
    "- Data di-split berdasarkan **hari** (bukan random), agar lebih realistis\n",
    "- Rasio: 60% train, 20% validation, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Temporal Split â€” sama persis dengan Multi-GNN data_loading.py\n",
    "# ============================================================\n",
    "\n",
    "n_days = df['day'].max() + 1\n",
    "n_samples = len(df)\n",
    "y = df['Is Laundering'].values\n",
    "\n",
    "# Calculate daily stats\n",
    "daily_trans = []\n",
    "daily_inds = []\n",
    "for day_idx in range(n_days):\n",
    "    day_mask = df['day'] == day_idx\n",
    "    day_indices = np.where(day_mask)[0]\n",
    "    daily_inds.append(day_indices)\n",
    "    daily_trans.append(len(day_indices))\n",
    "\n",
    "# Find optimal split\n",
    "split_per = [0.6, 0.2, 0.2]\n",
    "daily_totals = np.array(daily_trans)\n",
    "d_ts = daily_totals\n",
    "I = list(range(len(d_ts)))\n",
    "split_scores = {}\n",
    "\n",
    "for i, j in itertools.combinations(I, 2):\n",
    "    if j >= i:\n",
    "        split_totals = [d_ts[:i].sum(), d_ts[i:j].sum(), d_ts[j:].sum()]\n",
    "        split_totals_sum = np.sum(split_totals)\n",
    "        if split_totals_sum == 0:\n",
    "            continue\n",
    "        split_props = [v / split_totals_sum for v in split_totals]\n",
    "        split_error = [abs(v - t) / t for v, t in zip(split_props, split_per)]\n",
    "        score = max(split_error)\n",
    "        split_scores[(i, j)] = score\n",
    "\n",
    "i, j = min(split_scores, key=split_scores.get)\n",
    "split = [list(range(i)), list(range(i, j)), list(range(j, len(daily_totals)))]\n",
    "\n",
    "# Get indices\n",
    "tr_inds = np.concatenate([daily_inds[d] for d in split[0]]) if split[0] else np.array([], dtype=int)\n",
    "val_inds = np.concatenate([daily_inds[d] for d in split[1]]) if split[1] else np.array([], dtype=int)\n",
    "te_inds = np.concatenate([daily_inds[d] for d in split[2]]) if split[2] else np.array([], dtype=int)\n",
    "\n",
    "print(f'Number of days: {n_days}')\n",
    "print(f'Split days: Train={split[0][:5]}..., Val={split[1][:5]}..., Test={split[2][:5]}...')\n",
    "print(f'\\nTrain: {len(tr_inds)} samples ({len(tr_inds)/n_samples*100:.1f}%) | IR: {y[tr_inds].mean()*100:.2f}%')\n",
    "print(f'Val:   {len(val_inds)} samples ({len(val_inds)/n_samples*100:.1f}%) | IR: {y[val_inds].mean()*100:.2f}%')\n",
    "print(f'Test:  {len(te_inds)} samples ({len(te_inds)/n_samples*100:.1f}%) | IR: {y[te_inds].mean()*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a533950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Create Train / Val / Test DataFrames\n",
    "# ============================================================\n",
    "\n",
    "X = df[feature_cols]\n",
    "y_all = df['Is Laundering']\n",
    "\n",
    "X_train, y_train = X.iloc[tr_inds], y_all.iloc[tr_inds]\n",
    "X_val, y_val = X.iloc[val_inds], y_all.iloc[val_inds]\n",
    "X_test, y_test = X.iloc[te_inds], y_all.iloc[te_inds]\n",
    "\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_val:   {X_val.shape},   y_val:   {y_val.shape}')\n",
    "print(f'X_test:  {X_test.shape},  y_test:  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa2905d",
   "metadata": {},
   "source": [
    "## 5. Train CatBoost Model\n",
    "\n",
    "CatBoost dengan:\n",
    "- **`auto_class_weights='Balanced'`** untuk menangani class imbalance (sama seperti weighted CrossEntropy di Multi-GNN)\n",
    "- Early stopping pada validation set\n",
    "- Categorical features natively handled oleh CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CatBoost Training\n",
    "# ============================================================\n",
    "\n",
    "# Categorical features yang akan di-handle native oleh CatBoost\n",
    "cat_features = ['Payment Currency', 'Receiving Currency', 'Payment Format']\n",
    "\n",
    "# Convert to int for CatBoost categorical handling\n",
    "for col in cat_features:\n",
    "    X_train[col] = X_train[col].astype(int)\n",
    "    X_val[col] = X_val[col].astype(int)\n",
    "    X_test[col] = X_test[col].astype(int)\n",
    "\n",
    "# --- Manual scale_pos_weight (lebih terkontrol dari 'Balanced') ---\n",
    "# Multi-GNN menggunakan w_ce1=1.0, w_ce2=6.275 â†’ ratio ~6.3:1\n",
    "# 'Balanced' terlalu agresif untuk extreme imbalance (bisa 500:1)\n",
    "# Kita gunakan scale yg mirip Multi-GNN\n",
    "n_pos = y_train.sum()\n",
    "n_neg = len(y_train) - n_pos\n",
    "imbalance_ratio = n_neg / n_pos\n",
    "# Cap the weight to avoid over-prediction (mirip Multi-GNN w_ce2 ~6-9)\n",
    "scale_pos = 100.0 # Aggressively boosted\n",
    "print(f'Class imbalance: {n_neg}:{n_pos} = {imbalance_ratio:.1f}:1')\n",
    "print(f'Using scale_pos_weight: {scale_pos:.1f} (Aggressive for recall)')\n",
    "\n",
    "train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "val_pool = Pool(X_val, y_val, cat_features=cat_features)\n",
    "test_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.03,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=5,\n",
    "    scale_pos_weight=scale_pos,  # Controlled weight instead of 'Balanced'\n",
    "    eval_metric='F1',\n",
    "    random_seed=42,\n",
    "    verbose=200,\n",
    "    early_stopping_rounds=100,\n",
    "    task_type='CPU',\n",
    "    min_data_in_leaf=50,\n",
    "    random_strength=1,\n",
    "    bagging_temperature=1,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=val_pool,\n",
    "    use_best_model=True,\n",
    ")\n",
    "\n",
    "print(f'\\nBest iteration: {model.get_best_iteration()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a1a4a",
   "metadata": {},
   "source": [
    "## 6. Evaluasi Model\n",
    "\n",
    "Metrik yang sama dengan Multi-GNN:\n",
    "- F1-Score\n",
    "- Precision & Recall\n",
    "- PR-AUC (Precision-Recall Area Under Curve)\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, f1_score, precision_score, recall_score, average_precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def find_best_threshold(y_true, y_proba, metric='f2'):\n",
    "    \"\"\"Find the threshold that maximizes F2 score.\"\"\"\n",
    "    best_score = 0\n",
    "    best_thresh = 0.5\n",
    "    for thresh in np.arange(0.01, 0.95, 0.01):\n",
    "        y_pred_t = (y_proba >= thresh).astype(int)\n",
    "        if y_pred_t.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        if metric == 'f2':\n",
    "            score = fbeta_score(y_true, y_pred_t, beta=2)\n",
    "        else:\n",
    "            score = f1_score(y_true, y_pred_t)\n",
    "            \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thresh = thresh\n",
    "    return best_thresh, best_score\n",
    "\n",
    "def evaluate_model(model, X, y, dataset_name='Test', threshold=0.5):\n",
    "    \"\"\"Evaluate model with custom threshold.\"\"\"\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    \n",
    "    f1 = f1_score(y, y_pred)\n",
    "    f2 = fbeta_score(y, y_pred, beta=2)\n",
    "    precision = precision_score(y, y_pred, zero_division=0)\n",
    "    recall = recall_score(y, y_pred, zero_division=0)\n",
    "    pr_auc = average_precision_score(y, y_proba)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    print(f'=== {dataset_name} Metrics (threshold={threshold:.2f}) ===')\n",
    "    print(f'  F1:        {f1:.4f}')\n",
    "    print(f'  F2:        {f2:.4f}')\n",
    "    print(f'  Precision: {precision:.4f}')\n",
    "    print(f'  Recall:    {recall:.4f}')\n",
    "    print(f'  PR-AUC:    {pr_auc:.4f}')\n",
    "    print(f'  Confusion Matrix:')\n",
    "    print(f'    {cm}')\n",
    "    print()\n",
    "    \n",
    "    return {'f1': f1, 'f2': f2, 'precision': precision, 'recall': recall, \n",
    "            'pr_auc': pr_auc, 'confusion_matrix': cm, \n",
    "            'y_pred': y_pred, 'y_proba': y_proba}\n",
    "\n",
    "# --- Find optimal threshold on VALIDATION set ---\n",
    "val_proba = model.predict_proba(X_val)[:, 1]\n",
    "best_threshold, best_val_score = find_best_threshold(y_val, val_proba, metric='f2')\n",
    "print(f'Optimal threshold (tuned on validation for F2): {best_threshold:.2f} -> Val F2: {best_val_score:.4f}')\n",
    "print()\n",
    "\n",
    "# Evaluate on all splits with optimal threshold\n",
    "tr_metrics = evaluate_model(model, X_train, y_train, 'Train', threshold=best_threshold)\n",
    "val_metrics = evaluate_model(model, X_val, y_val, 'Validation', threshold=best_threshold)\n",
    "te_metrics = evaluate_model(model, X_test, y_test, 'Test', threshold=best_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c0037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Confusion Matrix Visualization\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, metrics, title in zip(axes, \n",
    "                               [tr_metrics, val_metrics, te_metrics],\n",
    "                               ['Train', 'Validation', 'Test']):\n",
    "    cm = metrics['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Legit', 'Laundering'],\n",
    "                yticklabels=['Legit', 'Laundering'])\n",
    "    ax.set_title(f'{title}\\nF1={metrics[\"f1\"]:.4f} | PR-AUC={metrics[\"pr_auc\"]:.4f}')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Precision-Recall Curve\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for metrics, label in [(val_metrics, 'Validation'), (te_metrics, 'Test')]:\n",
    "    y_true = y_val if label == 'Validation' else y_test\n",
    "    prec, rec, _ = precision_recall_curve(y_true, metrics['y_proba'])\n",
    "    ax.plot(rec, prec, label=f'{label} (PR-AUC={metrics[\"pr_auc\"]:.4f})')\n",
    "\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c508189",
   "metadata": {},
   "source": [
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Feature Importance\n",
    "# ============================================================\n",
    "\n",
    "feature_importance = model.get_feature_importance()\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "fi_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(6, len(feature_names) * 0.35)))\n",
    "ax.barh(fi_df['feature'], fi_df['importance'], color='steelblue')\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('CatBoost Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nTop 10 features:')\n",
    "print(fi_df.sort_values('importance', ascending=False).head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5e8ab",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806627f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Save Model\n",
    "# ============================================================\n",
    "\n",
    "MODEL_DIR = './saved_models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(MODEL_DIR, 'catboost_aml_model.cbm')\n",
    "model.save_model(model_path)\n",
    "print(f'Model saved to: {model_path}')\n",
    "\n",
    "# Save feature list for inference\n",
    "import json\n",
    "with open(os.path.join(MODEL_DIR, 'feature_cols.json'), 'w') as f:\n",
    "    json.dump(feature_cols, f)\n",
    "print(f'Feature columns saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Summary: CatBoost vs Multi-GNN Comparison\n",
    "# ============================================================\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Split': ['Train', 'Validation', 'Test'],\n",
    "    'F1': [tr_metrics['f1'], val_metrics['f1'], te_metrics['f1']],\n",
    "    'Precision': [tr_metrics['precision'], val_metrics['precision'], te_metrics['precision']],\n",
    "    'Recall': [tr_metrics['recall'], val_metrics['recall'], te_metrics['recall']],\n",
    "    'PR-AUC': [tr_metrics['pr_auc'], val_metrics['pr_auc'], te_metrics['pr_auc']],\n",
    "})\n",
    "\n",
    "print('=== CatBoost Results ===')\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print('\\n=== Multi-GNN Reference (GIN) ===')\n",
    "print('  Test F1: 0.2965 | Precision: 0.2376 | Recall: 0.3942 | PR-AUC: 0.2998')\n",
    "print(f'\\n=== Comparison ===')\n",
    "print(f'  CatBoost Test F1:  {te_metrics[\"f1\"]:.4f}')\n",
    "print(f'  Multi-GNN Test F1: 0.2965')\n",
    "print(f'  Î” F1: {te_metrics[\"f1\"] - 0.2965:+.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64044448",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Analisis Lanjutan untuk Skripsi\n",
    "\n",
    "Bagian ini berisi visualisasi dan analisis yang lebih mendalam untuk keperluan penulisan skripsi, meliputi:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)** â€” Distribusi dataset\n",
    "2. **Analisis Performa Model** â€” Metrik evaluasi dalam grafik thesis-quality\n",
    "3. **Perbandingan CatBoost vs Multi-GNN** â€” Tabel & chart komparatif\n",
    "4. **Analisis Threshold** â€” Dampak threshold terhadap precision-recall trade-off\n",
    "5. **Analisis Feature Importance** â€” Kontribusi fitur terhadap deteksi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ca93b",
   "metadata": {},
   "source": [
    "## 9.1 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93735a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.1.1 Distribusi Kelas (Class Distribution)\n",
    "# ============================================================\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12, 'axes.titlesize': 14, 'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 11, 'ytick.labelsize': 11, 'legend.fontsize': 11,\n",
    "    'figure.dpi': 150, 'savefig.dpi': 300, 'savefig.bbox': 'tight'\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), gridspec_kw={'width_ratios': [1.2, 1]})\n",
    "\n",
    "# --- Bar Chart ---\n",
    "class_counts = df['Is Laundering'].value_counts().sort_index()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = axes[0].bar(['Normal\\n(Kelas 0)', 'Mencurigakan\\n(Kelas 1)'], \n",
    "                    class_counts.values, color=colors, edgecolor='black', linewidth=0.8)\n",
    "axes[0].set_ylabel('Jumlah Transaksi')\n",
    "axes[0].set_title('Distribusi Kelas Transaksi', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 500,\n",
    "                f'{count:,}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "axes[0].set_ylim(0, class_counts.max() * 1.15)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# --- Pie Chart (Donut style, cleaner) ---\n",
    "labels = ['Normal', 'Mencurigakan']\n",
    "wedges, texts, autotexts = axes[1].pie(\n",
    "    class_counts.values, labels=None, autopct='%1.2f%%',\n",
    "    colors=colors, startangle=90, counterclock=False,\n",
    "    textprops={'fontsize': 12, 'fontweight': 'bold'},\n",
    "    pctdistance=0.78, wedgeprops=dict(width=0.55, edgecolor='white', linewidth=2)\n",
    ")\n",
    "\n",
    "# Add legend instead of inline labels to avoid clutter\n",
    "legend_labels = [f'{l}\\n({c:,})' for l, c in zip(labels, class_counts.values)]\n",
    "axes[1].legend(wedges, legend_labels, loc='center left', bbox_to_anchor=(0.9, 0.5),\n",
    "               fontsize=11, frameon=True, fancybox=True, shadow=True)\n",
    "axes[1].set_title('Proporsi Kelas Transaksi', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Dataset IBM Anti-Money Laundering (HI-Small)', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/eda_class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Total transaksi: {len(df):,}')\n",
    "print(f'Normal:       {class_counts[0]:,} ({class_counts[0]/len(df)*100:.2f}%)')\n",
    "print(f'Mencurigakan: {class_counts[1]:,} ({class_counts[1]/len(df)*100:.2f}%)')\n",
    "print(f'Rasio ketidakseimbangan: 1:{class_counts[0]//class_counts[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.1.2 Distribusi Jumlah Transaksi (Amount Distribution)\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# --- Histogram Amount Paid ---\n",
    "legit = df[df['Is Laundering'] == 0]['Amount Paid']\n",
    "illicit = df[df['Is Laundering'] == 1]['Amount Paid']\n",
    "\n",
    "axes[0,0].hist(legit, bins=100, alpha=0.7, label='Normal', color='#2ecc71', edgecolor='black', linewidth=0.3)\n",
    "axes[0,0].hist(illicit, bins=100, alpha=0.7, label='Mencurigakan', color='#e74c3c', edgecolor='black', linewidth=0.3)\n",
    "axes[0,0].set_xlabel('Amount Paid')\n",
    "axes[0,0].set_ylabel('Frekuensi')\n",
    "axes[0,0].set_title('Distribusi Amount Paid')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].set_xlim(0, df['Amount Paid'].quantile(0.99))\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "# --- Histogram Amount Received ---\n",
    "legit_r = df[df['Is Laundering'] == 0]['Amount Received']\n",
    "illicit_r = df[df['Is Laundering'] == 1]['Amount Received']\n",
    "\n",
    "axes[0,1].hist(legit_r, bins=100, alpha=0.7, label='Normal', color='#2ecc71', edgecolor='black', linewidth=0.3)\n",
    "axes[0,1].hist(illicit_r, bins=100, alpha=0.7, label='Mencurigakan', color='#e74c3c', edgecolor='black', linewidth=0.3)\n",
    "axes[0,1].set_xlabel('Amount Received')\n",
    "axes[0,1].set_ylabel('Frekuensi')\n",
    "axes[0,1].set_title('Distribusi Amount Received')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].set_xlim(0, df['Amount Received'].quantile(0.99))\n",
    "axes[0,1].grid(alpha=0.3)\n",
    "\n",
    "# --- Log Amount Box Plot by Class ---\n",
    "bp1 = axes[1,0].boxplot(\n",
    "    [np.log1p(legit), np.log1p(illicit)],\n",
    "    labels=['Normal', 'Mencurigakan'], patch_artist=True,\n",
    "    boxprops=dict(linewidth=1.2), medianprops=dict(color='black', linewidth=2)\n",
    ")\n",
    "bp1['boxes'][0].set_facecolor('#2ecc71')\n",
    "bp1['boxes'][1].set_facecolor('#e74c3c')\n",
    "axes[1,0].set_ylabel('Log(Amount Paid + 1)')\n",
    "axes[1,0].set_title('Box Plot: Amount Paid per Kelas')\n",
    "axes[1,0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "bp2 = axes[1,1].boxplot(\n",
    "    [np.log1p(legit_r), np.log1p(illicit_r)],\n",
    "    labels=['Normal', 'Mencurigakan'], patch_artist=True,\n",
    "    boxprops=dict(linewidth=1.2), medianprops=dict(color='black', linewidth=2)\n",
    ")\n",
    "bp2['boxes'][0].set_facecolor('#2ecc71')\n",
    "bp2['boxes'][1].set_facecolor('#e74c3c')\n",
    "axes[1,1].set_ylabel('Log(Amount Received + 1)')\n",
    "axes[1,1].set_title('Box Plot: Amount Received per Kelas')\n",
    "axes[1,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Analisis Distribusi Jumlah Transaksi', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/eda_amount_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Amount statistics\n",
    "print('=== Statistik Amount Paid ===')\n",
    "for label, data in [('Normal', legit), ('Mencurigakan', illicit)]:\n",
    "    print(f'  {label}: Mean={data.mean():.2f}, Median={data.median():.2f}, Std={data.std():.2f}, Max={data.max():.2f}')\n",
    "\n",
    "print('\\n=== Statistik Amount Received ===')\n",
    "for label, data in [('Normal', legit_r), ('Mencurigakan', illicit_r)]:\n",
    "    print(f'  {label}: Mean={data.mean():.2f}, Median={data.median():.2f}, Std={data.std():.2f}, Max={data.max():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baade0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.1.3 Distribusi Temporal â€” Transaksi per Hari & per Jam\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# --- Transaksi per hari (semua) ---\n",
    "daily_all = df.groupby('day').size()\n",
    "daily_illicit = df[df['Is Laundering'] == 1].groupby('day').size()\n",
    "\n",
    "axes[0,0].bar(daily_all.index, daily_all.values, color='#3498db', alpha=0.7, label='Total', edgecolor='black', linewidth=0.2)\n",
    "axes[0,0].set_xlabel('Hari ke-')\n",
    "axes[0,0].set_ylabel('Jumlah Transaksi')\n",
    "axes[0,0].set_title('Total Transaksi per Hari')\n",
    "axes[0,0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# --- Mencurigakan per hari ---\n",
    "axes[0,1].bar(daily_illicit.index, daily_illicit.values, color='#e74c3c', alpha=0.8, label='Mencurigakan', edgecolor='black', linewidth=0.2)\n",
    "axes[0,1].set_xlabel('Hari ke-')\n",
    "axes[0,1].set_ylabel('Jumlah Transaksi Mencurigakan')\n",
    "axes[0,1].set_title('Transaksi Mencurigakan per Hari')\n",
    "axes[0,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# --- Rasio mencurigakan per hari ---\n",
    "daily_rate = df.groupby('day')['Is Laundering'].mean() * 100\n",
    "axes[1,0].plot(daily_rate.index, daily_rate.values, color='#e67e22', linewidth=1.5, marker='o', markersize=3)\n",
    "axes[1,0].axhline(y=df['Is Laundering'].mean()*100, color='red', linestyle='--', alpha=0.5, label=f'Rata-rata ({df[\"Is Laundering\"].mean()*100:.2f}%)')\n",
    "axes[1,0].set_xlabel('Hari ke-')\n",
    "axes[1,0].set_ylabel('Rasio Mencurigakan (%)')\n",
    "axes[1,0].set_title('Rasio Transaksi Mencurigakan per Hari')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(alpha=0.3)\n",
    "\n",
    "# --- Distribusi per jam (hourly pattern) ---\n",
    "hourly_all = df.groupby('hour').size()\n",
    "hourly_illicit = df[df['Is Laundering'] == 1].groupby('hour').size()\n",
    "hourly_rate = df.groupby('hour')['Is Laundering'].mean() * 100\n",
    "\n",
    "ax2 = axes[1,1].twinx()\n",
    "axes[1,1].bar(hourly_all.index, hourly_all.values, color='#3498db', alpha=0.5, label='Total Transaksi')\n",
    "ax2.plot(hourly_rate.index, hourly_rate.values, color='#e74c3c', linewidth=2, marker='s', markersize=5, label='Rasio Mencurigakan (%)')\n",
    "axes[1,1].set_xlabel('Jam (0-23)')\n",
    "axes[1,1].set_ylabel('Jumlah Transaksi')\n",
    "ax2.set_ylabel('Rasio Mencurigakan (%)', color='#e74c3c')\n",
    "axes[1,1].set_title('Pola Transaksi per Jam')\n",
    "axes[1,1].set_xticks(range(0, 24))\n",
    "\n",
    "# Combined legend\n",
    "lines1, labels1 = axes[1,1].get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "axes[1,1].legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "axes[1,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Analisis Temporal Dataset', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/eda_temporal_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df035d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.1.4 Distribusi Payment Format & Currency\n",
    "# ============================================================\n",
    "\n",
    "# Reverse mapping for labels\n",
    "pf_reverse = {v: k for k, v in payment_format_map.items()}\n",
    "cur_reverse = {v: k for k, v in currency_map.items()}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# --- Payment Format ---\n",
    "pf_counts = df.groupby(['Payment Format', 'Is Laundering']).size().unstack(fill_value=0)\n",
    "pf_labels = [pf_reverse.get(i, str(i)) for i in pf_counts.index]\n",
    "x = np.arange(len(pf_labels))\n",
    "w = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - w/2, pf_counts[0], w, label='Normal', color='#2ecc71', edgecolor='black', linewidth=0.3)\n",
    "bars2 = axes[0].bar(x + w/2, pf_counts[1], w, label='Mencurigakan', color='#e74c3c', edgecolor='black', linewidth=0.3)\n",
    "axes[0].set_xlabel('Payment Format')\n",
    "axes[0].set_ylabel('Jumlah Transaksi')\n",
    "axes[0].set_title('Distribusi Payment Format per Kelas')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(pf_labels, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# --- Currency (Payment Currency) ---\n",
    "cur_counts = df.groupby(['Payment Currency', 'Is Laundering']).size().unstack(fill_value=0)\n",
    "cur_labels = [cur_reverse.get(i, str(i)) for i in cur_counts.index]\n",
    "# Top 10 currencies only\n",
    "if len(cur_labels) > 10:\n",
    "    top_cur = cur_counts.sum(axis=1).nlargest(10).index\n",
    "    cur_counts = cur_counts.loc[top_cur]\n",
    "    cur_labels = [cur_reverse.get(i, str(i)) for i in cur_counts.index]\n",
    "\n",
    "x2 = np.arange(len(cur_labels))\n",
    "bars3 = axes[1].bar(x2 - w/2, cur_counts[0], w, label='Normal', color='#2ecc71', edgecolor='black', linewidth=0.3)\n",
    "bars4 = axes[1].bar(x2 + w/2, cur_counts[1], w, label='Mencurigakan', color='#e74c3c', edgecolor='black', linewidth=0.3)\n",
    "axes[1].set_xlabel('Payment Currency')\n",
    "axes[1].set_ylabel('Jumlah Transaksi')\n",
    "axes[1].set_title('Distribusi Currency per Kelas (Top 10)')\n",
    "axes[1].set_xticks(x2)\n",
    "axes[1].set_xticklabels(cur_labels, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Distribusi Fitur Kategorikal', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/eda_categorical_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.1.5 Statistik Ringkasan Dataset (Tabel untuk Skripsi)\n",
    "# ============================================================\n",
    "\n",
    "# Dataset summary table\n",
    "dataset_summary = pd.DataFrame({\n",
    "    'Metrik': [\n",
    "        'Total Transaksi', 'Transaksi Normal', 'Transaksi Mencurigakan',\n",
    "        'Rasio Mencurigakan (%)', 'Rasio Ketidakseimbangan',\n",
    "        'Jumlah Akun Unik (Pengirim)', 'Jumlah Akun Unik (Penerima)', 'Jumlah Akun Unik (Total)',\n",
    "        'Jumlah Mata Uang', 'Jumlah Payment Format',\n",
    "        'Periode (Hari)', 'Jumlah Fitur (Engineered)'\n",
    "    ],\n",
    "    'Nilai': [\n",
    "        f'{len(df):,}',\n",
    "        f'{(df[\"Is Laundering\"]==0).sum():,}',\n",
    "        f'{(df[\"Is Laundering\"]==1).sum():,}',\n",
    "        f'{df[\"Is Laundering\"].mean()*100:.2f}%',\n",
    "        f'1:{(df[\"Is Laundering\"]==0).sum() // (df[\"Is Laundering\"]==1).sum()}',\n",
    "        f'{df[\"from_id\"].nunique():,}',\n",
    "        f'{df[\"to_id\"].nunique():,}',\n",
    "        f'{len(account_map):,}',\n",
    "        f'{len(currency_map)}',\n",
    "        f'{len(payment_format_map)}',\n",
    "        f'{df[\"day\"].max() + 1}',\n",
    "        f'{len(feature_cols)}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print('=' * 55)\n",
    "print('   RINGKASAN DATASET IBM AML (HI-Small)')\n",
    "print('=' * 55)\n",
    "print(dataset_summary.to_string(index=False))\n",
    "print('=' * 55)\n",
    "\n",
    "# Data split summary\n",
    "split_summary = pd.DataFrame({\n",
    "    'Split': ['Training', 'Validation', 'Test'],\n",
    "    'Jumlah': [len(tr_inds), len(val_inds), len(te_inds)],\n",
    "    'Proporsi (%)': [\n",
    "        f'{len(tr_inds)/len(df)*100:.1f}',\n",
    "        f'{len(val_inds)/len(df)*100:.1f}',\n",
    "        f'{len(te_inds)/len(df)*100:.1f}'\n",
    "    ],\n",
    "    'Mencurigakan': [y[tr_inds].sum(), y[val_inds].sum(), y[te_inds].sum()],\n",
    "    'Rasio Mencurigakan (%)': [\n",
    "        f'{y[tr_inds].mean()*100:.2f}',\n",
    "        f'{y[val_inds].mean()*100:.2f}',\n",
    "        f'{y[te_inds].mean()*100:.2f}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print('\\n')\n",
    "print('=' * 55)\n",
    "print('   PEMBAGIAN DATA (Temporal Split 60/20/20)')\n",
    "print('=' * 55)\n",
    "print(split_summary.to_string(index=False))\n",
    "print('=' * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc26fc2",
   "metadata": {},
   "source": [
    "## 9.2 Analisis Performa Model (Thesis-Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.2.1 Confusion Matrix â€” Thesis Quality (dengan Persentase)\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for ax, metrics, y_true, title in zip(\n",
    "    axes,\n",
    "    [tr_metrics, val_metrics, te_metrics],\n",
    "    [y_train, y_val, y_test],\n",
    "    ['Training Set', 'Validation Set', 'Test Set']\n",
    "):\n",
    "    cm = metrics['confusion_matrix']\n",
    "    cm_percent = cm.astype(float) / cm.sum() * 100\n",
    "    \n",
    "    # Annotate with count + percentage\n",
    "    annot = np.empty_like(cm, dtype=object)\n",
    "    for i_r in range(cm.shape[0]):\n",
    "        for j_c in range(cm.shape[1]):\n",
    "            annot[i_r, j_c] = f'{cm[i_r, j_c]:,}\\n({cm_percent[i_r, j_c]:.2f}%)'\n",
    "    \n",
    "    sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Prediksi\\nNormal', 'Prediksi\\nMencurigakan'],\n",
    "                yticklabels=['Aktual\\nNormal', 'Aktual\\nMencurigakan'],\n",
    "                linewidths=2, linecolor='white',\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    # Calculate specific rates\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) * 100\n",
    "    fnr = fn / (fn + tp) * 100\n",
    "    \n",
    "    ax.set_title(f'{title}\\nF1={metrics[\"f1\"]:.4f} | Prec={metrics[\"precision\"]:.4f} | Rec={metrics[\"recall\"]:.4f}\\n'\n",
    "                 f'FPR={fpr:.3f}% | FNR={fnr:.2f}%', fontsize=11)\n",
    "\n",
    "plt.suptitle('Confusion Matrix â€” Deteksi AML CatBoost', fontsize=15, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/thesis_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print detailed confusion matrix stats\n",
    "for name, metrics_d, y_t in [('Train', tr_metrics, y_train), ('Val', val_metrics, y_val), ('Test', te_metrics, y_test)]:\n",
    "    tn, fp, fn, tp = metrics_d['confusion_matrix'].ravel()\n",
    "    print(f'{name}: TP={tp:,}, TN={tn:,}, FP={fp:,}, FN={fn:,} | TPR={tp/(tp+fn)*100:.2f}%, FPR={fp/(fp+tn)*100:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53af7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.2.2 Precision-Recall Curve & ROC Curve (Side by Side)\n",
    "# ============================================================\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = {'Train': '#3498db', 'Validation': '#e67e22', 'Test': '#e74c3c'}\n",
    "\n",
    "# --- Precision-Recall Curve ---\n",
    "for metrics_d, y_true, label in [\n",
    "    (tr_metrics, y_train, 'Train'),\n",
    "    (val_metrics, y_val, 'Validation'),\n",
    "    (te_metrics, y_test, 'Test')\n",
    "]:\n",
    "    prec_arr, rec_arr, _ = precision_recall_curve(y_true, metrics_d['y_proba'])\n",
    "    ap = average_precision_score(y_true, metrics_d['y_proba'])\n",
    "    axes[0].plot(rec_arr, prec_arr, color=colors[label], linewidth=2, \n",
    "                label=f'{label} (AP={ap:.4f})')\n",
    "\n",
    "# Mark optimal threshold point on test\n",
    "best_prec = te_metrics['precision']\n",
    "best_rec = te_metrics['recall']\n",
    "axes[0].scatter([best_rec], [best_prec], s=150, c='red', marker='*', zorder=5, \n",
    "               label=f'Optimal Threshold ({best_threshold:.2f})')\n",
    "\n",
    "axes[0].set_xlabel('Recall', fontsize=12)\n",
    "axes[0].set_ylabel('Precision', fontsize=12)\n",
    "axes[0].set_title('Precision-Recall Curve', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(loc='upper right', fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim([0, 1.02])\n",
    "axes[0].set_ylim([0, 1.02])\n",
    "\n",
    "# --- ROC Curve ---\n",
    "for metrics_d, y_true, label in [\n",
    "    (tr_metrics, y_train, 'Train'),\n",
    "    (val_metrics, y_val, 'Validation'),\n",
    "    (te_metrics, y_test, 'Test')\n",
    "]:\n",
    "    fpr_arr, tpr_arr, _ = roc_curve(y_true, metrics_d['y_proba'])\n",
    "    auc_val = roc_auc_score(y_true, metrics_d['y_proba'])\n",
    "    axes[1].plot(fpr_arr, tpr_arr, color=colors[label], linewidth=2,\n",
    "                label=f'{label} (AUC={auc_val:.4f})')\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[1].set_title('ROC Curve', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(loc='lower right', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim([0, 1.02])\n",
    "axes[1].set_ylim([0, 1.02])\n",
    "\n",
    "plt.suptitle('Kurva Evaluasi Model CatBoost', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/thesis_pr_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463190fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.2.3 Analisis Threshold â€” Impact pada Precision, Recall, F1\n",
    "# ============================================================\n",
    "\n",
    "thresholds = np.arange(0.05, 0.96, 0.01)\n",
    "test_proba = te_metrics['y_proba']\n",
    "val_proba_arr = val_metrics['y_proba']\n",
    "\n",
    "# Compute metrics at each threshold for test set\n",
    "f1_scores, prec_scores, rec_scores = [], [], []\n",
    "for t in thresholds:\n",
    "    y_p = (test_proba >= t).astype(int)\n",
    "    if y_p.sum() == 0:\n",
    "        f1_scores.append(0)\n",
    "        prec_scores.append(0)\n",
    "        rec_scores.append(0)\n",
    "    else:\n",
    "        f1_scores.append(f1_score(y_test, y_p))\n",
    "        prec_scores.append(precision_score(y_test, y_p, zero_division=0))\n",
    "        rec_scores.append(recall_score(y_test, y_p, zero_division=0))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(thresholds, f1_scores, color='#2ecc71', linewidth=2.5, label='F1-Score')\n",
    "ax.plot(thresholds, prec_scores, color='#3498db', linewidth=2, label='Precision', linestyle='--')\n",
    "ax.plot(thresholds, rec_scores, color='#e74c3c', linewidth=2, label='Recall', linestyle='--')\n",
    "\n",
    "# Mark optimal threshold\n",
    "ax.axvline(x=best_threshold, color='#9b59b6', linestyle=':', linewidth=2.5, \n",
    "           label=f'Optimal Threshold = {best_threshold:.2f}')\n",
    "\n",
    "# Mark the F1 at optimal\n",
    "best_idx = np.argmin(np.abs(thresholds - best_threshold))\n",
    "ax.scatter([best_threshold], [f1_scores[best_idx]], s=200, c='#9b59b6', marker='*', zorder=5)\n",
    "ax.annotate(f'F1 = {f1_scores[best_idx]:.4f}', \n",
    "            xy=(best_threshold, f1_scores[best_idx]),\n",
    "            xytext=(best_threshold + 0.08, f1_scores[best_idx] + 0.05),\n",
    "            fontsize=11, fontweight='bold',\n",
    "            arrowprops=dict(arrowstyle='->', color='#9b59b6', lw=1.5))\n",
    "\n",
    "ax.set_xlabel('Threshold', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Analisis Threshold: Trade-off Precision vs Recall', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='center left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0.05, 0.95])\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/thesis_threshold_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print key threshold points\n",
    "print('=== Key Threshold Analysis (Test Set) ===')\n",
    "for t_check in [0.3, 0.4, 0.5, best_threshold, 0.7, 0.8, 0.9]:\n",
    "    idx = np.argmin(np.abs(thresholds - t_check))\n",
    "    print(f'  Threshold={thresholds[idx]:.2f}: F1={f1_scores[idx]:.4f}, Precision={prec_scores[idx]:.4f}, Recall={rec_scores[idx]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.2.4 Feature Importance â€” Top 20 (Thesis Quality)\n",
    "# ============================================================\n",
    "\n",
    "# Get SHAP-like feature importance (PredictionValuesChange)\n",
    "fi_pred = model.get_feature_importance(type='PredictionValuesChange')\n",
    "fi_loss = model.get_feature_importance(type='LossFunctionChange', data=test_pool)\n",
    "\n",
    "fi_combined = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Prediction Values Change': fi_pred,\n",
    "    'Loss Function Change': fi_loss\n",
    "}).sort_values('Prediction Values Change', ascending=False)\n",
    "\n",
    "# Top 20\n",
    "top_n = 20\n",
    "fi_top = fi_combined.head(top_n).sort_values('Prediction Values Change', ascending=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# --- PredictionValuesChange ---\n",
    "colors_fi = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, top_n))[::-1]\n",
    "axes[0].barh(fi_top['Feature'], fi_top['Prediction Values Change'], \n",
    "             color=colors_fi, edgecolor='black', linewidth=0.3)\n",
    "axes[0].set_xlabel('Importance Score', fontsize=11)\n",
    "axes[0].set_title('Feature Importance\\n(Prediction Values Change)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# --- LossFunctionChange ---\n",
    "fi_top2 = fi_combined.sort_values('Loss Function Change', ascending=False).head(top_n).sort_values('Loss Function Change', ascending=True)\n",
    "colors_fi2 = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, top_n))[::-1]\n",
    "axes[1].barh(fi_top2['Feature'], fi_top2['Loss Function Change'],\n",
    "             color=colors_fi2, edgecolor='black', linewidth=0.3)\n",
    "axes[1].set_xlabel('Importance Score', fontsize=11)\n",
    "axes[1].set_title('Feature Importance\\n(Loss Function Change)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Top {top_n} Fitur Terpenting â€” CatBoost AML', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/thesis_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print ranking table\n",
    "print(f'\\n=== Top {top_n} Feature Importance Ranking ===')\n",
    "fi_rank = fi_combined.head(top_n).reset_index(drop=True)\n",
    "fi_rank.index = fi_rank.index + 1\n",
    "fi_rank.index.name = 'Rank'\n",
    "print(fi_rank.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873e951",
   "metadata": {},
   "source": [
    "## 9.3 Perbandingan CatBoost vs Multi-GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b97591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.3.1 Bar Chart Perbandingan Metrik CatBoost vs Multi-GNN\n",
    "# ============================================================\n",
    "\n",
    "# Multi-GNN reference results (GIN model, dari paper/eksperimen)\n",
    "multignn_results = {\n",
    "    'F1-Score': 0.2965,\n",
    "    'Precision': 0.2376,\n",
    "    'Recall': 0.3942,\n",
    "    'PR-AUC': 0.2998\n",
    "}\n",
    "\n",
    "catboost_results = {\n",
    "    'F1-Score': te_metrics['f1'],\n",
    "    'Precision': te_metrics['precision'],\n",
    "    'Recall': te_metrics['recall'],\n",
    "    'PR-AUC': te_metrics['pr_auc']\n",
    "}\n",
    "\n",
    "metrics_names = list(multignn_results.keys())\n",
    "multignn_vals = list(multignn_results.values())\n",
    "catboost_vals = list(catboost_results.values())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, multignn_vals, width, label='Multi-GNN (GIN)', \n",
    "               color='#e74c3c', edgecolor='black', linewidth=0.8, alpha=0.85)\n",
    "bars2 = ax.bar(x + width/2, catboost_vals, width, label='CatBoost (Ours)', \n",
    "               color='#2ecc71', edgecolor='black', linewidth=0.8, alpha=0.85)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{bar.get_height():.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Perbandingan Performa: CatBoost vs Multi-GNN (Test Set)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_names, fontsize=12)\n",
    "ax.legend(fontsize=12, loc='upper left')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/thesis_comparison_bar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb25312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.3.2 Radar Chart â€” Perbandingan Multi-Dimensi\n",
    "# ============================================================\n",
    "\n",
    "categories = ['F1-Score', 'Precision', 'Recall', 'PR-AUC']\n",
    "N = len(categories)\n",
    "\n",
    "# Values\n",
    "values_gnn = [0.2965, 0.2376, 0.3942, 0.2998]\n",
    "values_cb = [te_metrics['f1'], te_metrics['precision'], te_metrics['recall'], te_metrics['pr_auc']]\n",
    "\n",
    "# Close the radar\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "values_gnn += values_gnn[:1]\n",
    "values_cb += values_cb[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "ax.plot(angles, values_gnn, 'o-', linewidth=2.5, color='#e74c3c', label='Multi-GNN (GIN)', markersize=8)\n",
    "ax.fill(angles, values_gnn, alpha=0.15, color='#e74c3c')\n",
    "\n",
    "ax.plot(angles, values_cb, 'o-', linewidth=2.5, color='#2ecc71', label='CatBoost (Ours)', markersize=8)\n",
    "ax.fill(angles, values_cb, alpha=0.15, color='#2ecc71')\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=12, fontweight='bold')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9, alpha=0.7)\n",
    "ax.set_title('Radar Chart: CatBoost vs Multi-GNN', fontsize=14, fontweight='bold', y=1.1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/thesis_radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8816075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.3.3 Tabel Perbandingan Lengkap (untuk Bab Hasil Skripsi)\n",
    "# ============================================================\n",
    "\n",
    "# Improvement calculation\n",
    "improvements = {}\n",
    "for metric in ['F1-Score', 'Precision', 'Recall', 'PR-AUC']:\n",
    "    gnn_val = multignn_results[metric]\n",
    "    cb_val = catboost_results[metric]\n",
    "    delta = cb_val - gnn_val\n",
    "    pct_change = (delta / gnn_val) * 100\n",
    "    improvements[metric] = {'delta': delta, 'pct': pct_change}\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metrik': ['F1-Score', 'Precision', 'Recall', 'PR-AUC'],\n",
    "    'Multi-GNN (GIN)': [f'{v:.4f}' for v in multignn_vals[:4]],\n",
    "    'CatBoost': [f'{v:.4f}' for v in catboost_vals[:4]],\n",
    "    'Î” (Selisih)': [f'{improvements[m][\"delta\"]:+.4f}' for m in ['F1-Score', 'Precision', 'Recall', 'PR-AUC']],\n",
    "    'Peningkatan (%)': [f'{improvements[m][\"pct\"]:+.1f}%' for m in ['F1-Score', 'Precision', 'Recall', 'PR-AUC']]\n",
    "})\n",
    "\n",
    "print('=' * 75)\n",
    "print('   TABEL PERBANDINGAN PERFORMA MODEL (Test Set)')\n",
    "print('   Dataset: IBM AML (HI-Small) | Split: Temporal 60/20/20')\n",
    "print('=' * 75)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print('=' * 75)\n",
    "\n",
    "# Additional context\n",
    "print(f'\\nCatatan Penting:')\n",
    "print(f'  - Multi-GNN: arsitektur GINe + LinkNeighborLoader')\n",
    "print(f'  - CatBoost: 2000 iterasi, depth=8, lr=0.03')\n",
    "print(f'  - Kedua model menggunakan temporal split identik')\n",
    "print(f'  - CatBoost menggunakan optimasi threshold ({best_threshold:.2f})')\n",
    "print(f'  - Multi-GNN menggunakan threshold default (0.5)')\n",
    "\n",
    "# CatBoost across splits table\n",
    "print(f'\\n')\n",
    "print('=' * 65)\n",
    "print('   PERFORMA CATBOOST PER SPLIT')\n",
    "print('=' * 65)\n",
    "\n",
    "splits_df = pd.DataFrame({\n",
    "    'Split': ['Training', 'Validation', 'Test'],\n",
    "    'F1-Score': [f'{tr_metrics[\"f1\"]:.4f}', f'{val_metrics[\"f1\"]:.4f}', f'{te_metrics[\"f1\"]:.4f}'],\n",
    "    'Precision': [f'{tr_metrics[\"precision\"]:.4f}', f'{val_metrics[\"precision\"]:.4f}', f'{te_metrics[\"precision\"]:.4f}'],\n",
    "    'Recall': [f'{tr_metrics[\"recall\"]:.4f}', f'{val_metrics[\"recall\"]:.4f}', f'{te_metrics[\"recall\"]:.4f}'],\n",
    "    'PR-AUC': [f'{tr_metrics[\"pr_auc\"]:.4f}', f'{val_metrics[\"pr_auc\"]:.4f}', f'{te_metrics[\"pr_auc\"]:.4f}'],\n",
    "})\n",
    "print(splits_df.to_string(index=False))\n",
    "print('=' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.3.4 Performa CatBoost per Split (Grouped Bar Chart)\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "splits = ['Training', 'Validation', 'Test']\n",
    "metrics_list = ['F1-Score', 'Precision', 'Recall', 'PR-AUC']\n",
    "\n",
    "data_per_split = {\n",
    "    'Training': [tr_metrics['f1'], tr_metrics['precision'], tr_metrics['recall'], tr_metrics['pr_auc']],\n",
    "    'Validation': [val_metrics['f1'], val_metrics['precision'], val_metrics['recall'], val_metrics['pr_auc']],\n",
    "    'Test': [te_metrics['f1'], te_metrics['precision'], te_metrics['recall'], te_metrics['pr_auc']],\n",
    "}\n",
    "\n",
    "x = np.arange(len(metrics_list))\n",
    "width = 0.25\n",
    "colors_split = ['#3498db', '#e67e22', '#2ecc71']\n",
    "\n",
    "for i, (split_name, vals) in enumerate(data_per_split.items()):\n",
    "    bars = ax.bar(x + i * width, vals, width, label=split_name, \n",
    "                  color=colors_split[i], edgecolor='black', linewidth=0.5, alpha=0.85)\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Performa CatBoost per Data Split', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(metrics_list, fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/thesis_catboost_splits.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b7b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9.4 Korelasi Fitur dengan Label (Top Features)\n",
    "# ============================================================\n",
    "\n",
    "# Correlation with target\n",
    "corr_with_label = df[feature_cols + ['Is Laundering']].corr()['Is Laundering'].drop('Is Laundering')\n",
    "corr_sorted = corr_with_label.abs().sort_values(ascending=False).head(20)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# --- Top 20 correlation bar ---\n",
    "top_corr = corr_with_label[corr_sorted.index].sort_values()\n",
    "colors_corr = ['#e74c3c' if v < 0 else '#2ecc71' for v in top_corr.values]\n",
    "axes[0].barh(top_corr.index, top_corr.values, color=colors_corr, edgecolor='black', linewidth=0.3)\n",
    "axes[0].axvline(x=0, color='black', linewidth=0.8)\n",
    "axes[0].set_xlabel('Pearson Correlation', fontsize=11)\n",
    "axes[0].set_title('Top 20 Fitur: Korelasi dengan Label\\n(Is Laundering)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# --- Correlation heatmap of top features + label ---\n",
    "top_feat_names = corr_sorted.index[:12].tolist()\n",
    "corr_matrix = df[top_feat_names + ['Is Laundering']].corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, ax=axes[1], linewidths=0.5,\n",
    "            annot_kws={'size': 8}, square=True,\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "axes[1].set_title('Heatmap Korelasi\\n(Top 12 Fitur + Label)', fontsize=12, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('Analisis Korelasi Fitur', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./saved_models/thesis_correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nTop 10 fitur dengan korelasi tertinggi terhadap Is Laundering:')\n",
    "for i, (feat, val) in enumerate(corr_sorted.head(10).items(), 1):\n",
    "    direction = 'â†‘' if corr_with_label[feat] > 0 else 'â†“'\n",
    "    print(f'  {i:2d}. {feat:30s} r = {corr_with_label[feat]:+.4f} {direction}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
